{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Team_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrey1234/politifact-analysis/blob/master/Team_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBY93kh1dkto",
        "outputId": "054e5bab-e0f1-4580-acb8-d39d4e030687"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMJVaVe6e47V"
      },
      "source": [
        "##Social Credibility - Shreya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQdR5hB2fAv-"
      },
      "source": [
        "### Sentiment polarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG5fJyC6fFXm",
        "outputId": "7c1ef3ce-0ea2-4b96-c359-9216fce09748"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "import nltk.sentiment\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import warnings\n",
        "import seaborn as sb\n",
        "from pprint import pprint\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYDhT4c4fJu4"
      },
      "source": [
        "def preprocess_text(document):    \n",
        "    # 1. Remove non-letters/Special Characters and Punctuations\n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", document)\n",
        "    \n",
        "    # 2. Convert to lower case.\n",
        "    news =  news.lower()\n",
        "    \n",
        "    # 3. Tokenize.\n",
        "    news_words = nltk.word_tokenize( news)\n",
        "    \n",
        "    # 4. Convert the stopwords list to \"set\" data type.\n",
        "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    \n",
        "    # 5. Remove stop words. \n",
        "    words = [w for w in  news_words  if not w in stops]\n",
        "    \n",
        "    # 6. Lemmatize \n",
        "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "    \n",
        "    # 7. Stemming\n",
        "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "    \n",
        "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "    return \" \".join(stems)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWS9UyCPfVHV"
      },
      "source": [
        "def get_vader_polarity(snt):\n",
        "    if not snt:\n",
        "        return None\n",
        "    elif snt['neg'] > snt['pos'] and snt['neg'] > snt['neu']:\n",
        "        return -1\n",
        "    elif snt['pos'] > snt['neg'] and snt['pos'] > snt['neu']:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def get_polarity_type(sentence):\n",
        "    sentimentVector = []\n",
        "    senti = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
        "    snt = senti.polarity_scores(sentence)\n",
        "    sentimentVector.append(get_vader_polarity(snt))\n",
        "    sentimentVector.append(snt['neg'])\n",
        "    sentimentVector.append(snt['neu'])\n",
        "    sentimentVector.append(snt['pos'])\n",
        "    sentimentVector.append(snt['compound'])\n",
        "    \n",
        "    return sentimentVector\n",
        "\n",
        "def get_sentiment(df,column):\n",
        "  df['processed'] = df[column].apply(preprocess_text) \n",
        "  sentiment = []\n",
        "  vader_pol = []\n",
        "  cmp_score = []\n",
        "  for row in df['processed']:\n",
        "      get_pols = get_polarity_type(row)\n",
        "      sentiment.append(get_pols[1:])\n",
        "      vader_pol.append(get_pols[0])\n",
        "      cmp_score.append(get_pols[1:][-1])\n",
        "    \n",
        "  df['vader_polarity'] = vader_pol\n",
        "  df['sentiment_score'] = cmp_score \n",
        "  df = df[['vader_polarity','sentiment_score']]\n",
        "  return df   "
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DeKAjcBfhDA"
      },
      "source": [
        "### Topic credibility\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp4mjYFXfk0c"
      },
      "source": [
        "import gensim \n",
        "\n",
        "def get_word_tokens(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if len(token) > 3:\n",
        "            result.append(token)\n",
        "    return result\n",
        "# tokenized_docs_local = df['processed'].map(get_word_tokens)\n",
        "\n",
        "def get_dictionary_print_words(dataframe,colname):\n",
        "    processed_docs = dataframe['processed'].map(lambda doc: doc.split(\" \"))\n",
        "    dictionary_gensim = gensim.corpora.Dictionary(processed_docs)\n",
        "    count = 0\n",
        "    print('######## DICTIONARY Words and occurences ########')\n",
        "    for k, v in dictionary_gensim.iteritems():\n",
        "        print(k, v)\n",
        "        count += 1\n",
        "        if count > 10:\n",
        "            break\n",
        "    #dictionary_gensim.filter_extremes(no_below=100, no_above=0, keep_n=100000)\n",
        "    return dictionary_gensim\n",
        "\n",
        "def get_lda_model_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n",
        "    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2)\n",
        "    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=10,formatted=False)\n",
        "    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n",
        "\n",
        "    #Below Code Prints Topics and Words\n",
        "    for topic,words in lda_topics_words:\n",
        "        print(str(topic)+ \"::\"+ str(words))\n",
        "    return lda_model"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CUfoxXNf2EL"
      },
      "source": [
        "def get_lda_model_topics_topwords_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n",
        "    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2, random_state=1)\n",
        "    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=10,formatted=False)\n",
        "    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n",
        "\n",
        "    #Below Code Prints Topics and Words\n",
        "    for topic,words in lda_topics_words:\n",
        "        print(str(topic)+ \"::\"+ str(words))\n",
        "    return lda_model,lda_topics_words\n",
        "def identify_topic_number_score_label_topwords(text,dictionary_local,lda_model_local,lda_topics_top_words_local):\n",
        "    bow_vector_local = dictionary_local.doc2bow(get_word_tokens(text))\n",
        "    topic_number_local, topic_score_local = sorted(\n",
        "        lda_model_local[bow_vector_local], key=lambda tup: -1*tup[1])[0]\n",
        "    #print (topic_number_local, topic_score_local)\n",
        "    return pd.Series([topic_number_local, topic_score_local,\" \".join(lda_topics_top_words_local[int(topic_number_local)][1])])\n",
        "\n",
        "def update_lda_results_to_dataset(dataframe,topiccolnames,coltoapplylda,colnamedictionary,colnameldamodel, colnameldatopwords):\n",
        "    dataframe[topiccolnames] = dataframe.apply(\n",
        "    lambda row: identify_topic_number_score_label_topwords(\n",
        "        row[coltoapplylda],colnamedictionary,colnameldamodel,\n",
        "        colnameldatopwords), axis=1)\n",
        "    return dataframe"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqbViua1f7pO"
      },
      "source": [
        "def get_bow_corpus_print_sample(dataframe,colname):\n",
        "    tokenized_docs_local = dataframe['processed'].map(get_word_tokens)\n",
        "    dictionary_gensim = get_dictionary_print_words(dataframe, colname)\n",
        "    bow_corpus_local = [dictionary_gensim.doc2bow(doc) for doc in tokenized_docs_local]\n",
        "    bow_doc_local_0 = bow_corpus_local[0]\n",
        "    print('\\n ######## BOW VECTOR FIRST ITEM ########')\n",
        "    print(bow_doc_local_0)\n",
        "    print('\\n ######## PREVIEW BOW ########')\n",
        "    for i in range(len(bow_doc_local_0)):\n",
        "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_local_0[i][0], \n",
        "                                               dictionary_gensim[bow_doc_local_0[i][0]], bow_doc_local_0[i][1]))\n",
        "    return bow_corpus_local, dictionary_gensim\n",
        "def get_tfidf_corpus_print_sample(bow_corpus_local):\n",
        "    from gensim import corpora, models\n",
        "    tfidf = models.TfidfModel(bow_corpus_local)\n",
        "    tfidf_corpus_local = tfidf[bow_corpus_local]\n",
        "    print('\\n ######## TFIDF VECTOR FIRST ITEM ########')\n",
        "    \n",
        "    from pprint import pprint\n",
        "    for doc in tfidf_corpus_local:\n",
        "        pprint(doc)\n",
        "        break\n",
        "    return tfidf_corpus_local    "
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ma2TmKtf-hU"
      },
      "source": [
        "def topic_modelling(df,column):\n",
        "  df['processed'] = df[column].apply(preprocess_text) \n",
        "  bow_corpus_headline, dictionary_headline = get_bow_corpus_print_sample(df,'processed')\n",
        "  lda_model_headline, lda_headline_topic_words = get_lda_model_topics_topwords_print_top_topics(\n",
        "        bow_corpus_headline, 10 ,dictionary_headline)\n",
        "  headlinetopiccolnames = ['topic_number','lda_score','topic_top_words']\n",
        "  fake_newss = update_lda_results_to_dataset(df, headlinetopiccolnames,'processed', dictionary_headline, lda_model_headline, lda_headline_topic_words)\n",
        "  # trained_df = fake_newss[['topic_number','lda_score']]\n",
        "  return fake_newss"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZT_T509n2jj"
      },
      "source": [
        "# topic_modelling(d,'tweet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX28WqJIgk_M"
      },
      "source": [
        "### Content of post"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwvn2JFVgraJ"
      },
      "source": [
        "def getContent(df):\n",
        "  X = df[['user_mentions','retweet_count','favorite_count','hashtags_count']]\n",
        "  #X = df['symbol_count','hashtags_count']\n",
        "\n",
        "  return X"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykS1-5Frhr3f"
      },
      "source": [
        "### Combined model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfh0YqgVhv32"
      },
      "source": [
        "#Best pickled model for sentiment analysis\n",
        "import pickle\n",
        "\n",
        "def getSocialScore(df2 , column):\n",
        "\n",
        "  # extracting feature analysis features for model\n",
        "  trained_df2 = get_sentiment(df2,column)\n",
        "  sentiment_model ='/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Shreya/Week 9/SentimentModel.pkl'\n",
        "  loaded_model = pickle.load(open(sentiment_model, 'rb'))\n",
        "  predicted_sentiment = loaded_model.predict(trained_df2)\n",
        "  predicted_scores = loaded_model.predict_proba(trained_df2)\n",
        "\n",
        "  df2.loc[:,'sentiment'] = predicted_sentiment\n",
        "  df2.loc[:,'senti_0'] = predicted_scores[:,0]\n",
        "  df2.loc[:,'senti_1'] = predicted_scores[:,1]\n",
        "  df2 = df2[[column,'sentiment','senti_0','senti_1','user_mentions','retweet_count','favorite_count','hashtags_count','tweet_id']]\n",
        "\n",
        "  #Best pickled model for topic credibility\n",
        "\n",
        "  # extracting topic credibility features for model\n",
        "  trained_df = topic_modelling(df2,column)\n",
        "  X = df2[['topic_number','lda_score']]\n",
        "  topic_model = '/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Shreya/Week 9/TopicCredibility.pkl'\n",
        "  loaded_model = pickle.load(open(topic_model, 'rb'))\n",
        "  predicted_sentiment = loaded_model.predict(X)\n",
        "  predicted_scores = loaded_model.predict_proba(X)\n",
        "\n",
        "  df2.loc[:,'topic'] = predicted_sentiment\n",
        "  df2.loc[:,'topic_0'] = predicted_scores[:,0]\n",
        "  df2.loc[:,'topic_1'] = predicted_scores[:,1]\n",
        "\n",
        "  # extracting features for model\n",
        "  #trained_df3 = getContent(df2)\n",
        "  print(\"inside content modelling\")\n",
        "  trained_df3 = df2[['user_mentions','retweet_count','favorite_count','hashtags_count']]\n",
        "  content_model = '/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Shreya/Week 9/Retweet_HashtagModel.pkl'\n",
        "  loaded_model = pickle.load(open(content_model, 'rb'))\n",
        "  predicted_content = loaded_model.predict(trained_df3)\n",
        "  df2.loc[:,'content'] = predicted_content\n",
        "\n",
        "  print(df2.columns)\n",
        "\n",
        "  #result_df = df2[[column,'sentiment','senti_0','senti_1','topic_0','topic_1','content']]\n",
        "  result_df = df2[[column,'senti_0','topic_0','content','tweet_id']]\n",
        "  getScore(result_df)\n",
        "  return getScore(result_df)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5aOn3rmw8y0"
      },
      "source": [
        "def getScore(result_df):\n",
        "  result_df['content'] = result_df['content'].apply(pd.to_numeric)      \n",
        "  result_df['social_credibility'] = result_df['senti_0']*0.5 + result_df['topic_0']*0.2 + result_df['content']*0.3\n",
        "  result_df = result_df[['tweet_id','tweet','social_credibility']]\n",
        "  #result_df['type'] = result_df['model_acc'].apply(complex_function) \n",
        "  return result_df"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYRs9X7KXrOh"
      },
      "source": [
        "## Content Veracity -Rishitha\n",
        "### Micro factors \n",
        "**structural features**\n",
        "* tweet_length\n",
        "* symbol_count\n",
        "* urls_count\n",
        "* media_count\n",
        "* has_smile_emoji\n",
        "* sensitive\n",
        "* has_quest_or_exclaim\n",
        "* number_punct\n",
        "* capitalratio \n",
        "\n",
        "**content features**\n",
        "* contentlength\n",
        "* negativewordcount\n",
        "* positivewordcount\n",
        "* Noun\n",
        "* Verb\n",
        "* Adjective\n",
        "* Adverb\n",
        "* Pronoun\n",
        "* FirstPersonPronoun \n",
        "* SecondPersonPronoun\n",
        "* ThirdPersonPronoun\n",
        "\n",
        "**emotional features**\n",
        "* sentimentscore\n",
        "\n",
        "##Datasets\n",
        "* PHEME dataset(tweets which are labelled)\n",
        "* Scraped Tweets using Tweepy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc0Z5WGWYqbm"
      },
      "source": [
        "### Combined features for training on the PHEME dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KDG6SWTYBnM",
        "outputId": "9402f24f-73c9-4230-b6e0-a9b2d06a5eff"
      },
      "source": [
        "import os, json, errno\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sys import argv\n",
        "import string\n",
        "import time\n",
        "from multiprocessing import Process\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re\n",
        "from nltk.corpus import stopwords as stp\n",
        "from textblob import TextBlob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def to_unix_tmsp(col):\n",
        "    return pd.DatetimeIndex(col).astype(np.int64) / 1e6\n",
        "\n",
        "def parse_twitter_datetime(timestr):\n",
        "    return pd.datetime.strptime(timestr, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "class Tweets:\n",
        "\n",
        "    def __init__(self, event_name, output_dir=\"data/tweets\"):\n",
        "        self.event = event_name\n",
        "        self.data = {}\n",
        "        self.output_dir = output_dir\n",
        "        self.printable = set(string.printable)\n",
        "    \n",
        "    def append(self, twt, cat, thrd, is_src):\n",
        "        twt['category'] = cat\n",
        "        twt[\"thread\"] = thrd\n",
        "        twt[\"event\"] = self.event\n",
        "        twt[\"is_src\"] = is_src\n",
        "\n",
        "        twt_text=twt[\"text\"]\n",
        "        twt_text_filtered=str()\n",
        "        for c in twt_text:\n",
        "            if c in self.printable:\n",
        "                twt_text_filtered+=c\n",
        "        text_features=self.tweettext2features(twt_text_filtered)\n",
        "        has_question = \"?\" in twt[\"text\"]\n",
        "        has_exclaim = \"!\" in twt[\"text\"]\n",
        "        twt['twt_text_filtered']=twt_text_filtered\n",
        "\n",
        "        features = {\n",
        "            \"is_rumour\": lambda obj : 1 if obj['category'] == \"rumours\" else 0,\n",
        "            \"tweet_id\" : lambda obj : obj.get(\"id\"),\n",
        "            \"tweet\": lambda obj: obj.get('text'),\n",
        "            \"filtered_tweet\": lambda obj: obj.get('twt_text_filtered'),\n",
        "            \"event\" : lambda obj : obj.get(\"event\"),\n",
        "            \"thread\" : lambda obj : obj[\"thread\"],\n",
        "            #structural features\n",
        "            \"tweet_length\": lambda obj : len(obj.get(\"text\",\"\")),\n",
        "            \"symbol_count\": lambda obj: len(obj[\"entities\"].get(\"symbols\", [])),\n",
        "            \"user_mentions\": lambda obj: len(obj[\"entities\"].get(\"user_mentions\", [])),\n",
        "            \"urls_count\": lambda obj : len(obj[\"entities\"].get(\"urls\", [])),\n",
        "            \"media_count\": lambda obj: len(obj[\"entities\"].get(\"media\", [])),\n",
        "            \"hashtags_count\": lambda obj : len(obj[\"entities\"].get(\"hashtags\", [])),\n",
        "            \"retweet_count\": lambda obj : obj.get(\"retweet_count\", 0),\n",
        "            \"favorite_count\": lambda obj : obj.get(\"favorite_count\"),\n",
        "            \"mentions_count\": lambda obj : len(obj[\"entities\"].get(\"user_mentions\", \"\")),\n",
        "            \"has_smile_emoji\": lambda obj: 1 if \"😊\" in obj[\"text\"] else 0,\n",
        "            \"has_place\": lambda obj: 1 if obj.get(\"place\") else 0,\n",
        "            \"has_coords\": lambda obj: 1 if obj.get(\"coordinates\") else 0,\n",
        "            \"has_quest\": lambda obj: 1 if has_question else 0,\n",
        "            \"has_exclaim\": lambda obj: 1 if has_exclaim else 0,\n",
        "            \"has_quest_or_exclaim\": lambda obj: 1 if (has_question or has_exclaim) else 0,\n",
        "            \"sensitive\": lambda obj: 1 if obj.get(\"possibly_sensitive\") else 0,            \n",
        "\n",
        "        }\n",
        "\n",
        "        for col in features:\n",
        "            self.data.setdefault(col, []).append(features[col](twt))\n",
        "\n",
        "        for col in text_features:\n",
        "            self.data.setdefault(col, []).append(text_features[col])\n",
        "\n",
        "    def tweettext2features(self, tweet_text):   \n",
        "        def punctuationanalysis(tweet_text):\n",
        "            punctuations= [\"\\\"\",\"(\",\")\",\"*\",\",\",\"-\",\"_\",\".\",\"~\",\"%\",\"^\",\"&\",\"!\",\"#\",'@'\n",
        "               \"=\",\"\\'\",\"\\\\\",\"+\",\"/\",\":\",\"[\",\"]\",\"«\",\"»\",\"،\",\"؛\",\"?\",\".\",\"…\",\"$\",\n",
        "               \"|\",\"{\",\"}\",\"٫\",\";\",\">\",\"<\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]\n",
        "            hasperiod=sum(c =='.' for c in tweet_text)\n",
        "            number_punct=sum(c in punctuations for c in tweet_text)\n",
        "            return {'hasperiod':hasperiod,'number_punct':number_punct}\n",
        "\n",
        "        def negativewordcount(tokens):\n",
        "            count = 0\n",
        "            negativeFeel = ['tired', 'sick', 'bord', 'uninterested', 'nervous', 'stressed',\n",
        "                            'afraid', 'scared', 'frightened', 'boring','bad',\n",
        "                            'distress', 'uneasy', 'angry', 'annoyed', 'pissed',\"hate\",\n",
        "                            'sad', 'bitter', 'down', 'depressed', 'unhappy','heartbroken','jealous', 'fake', 'stupid', 'strange','absurd', 'crazy']\n",
        "            for negative in negativeFeel:\n",
        "                if negative in tokens:\n",
        "                    count += 1\n",
        "            return count\n",
        "\n",
        "        def positivewordcount(tokens):\n",
        "            count = 0\n",
        "            positivewords = ['joy', ' happy', 'hope', 'kind', 'surprise'\n",
        "                            , 'excite', ' interest', 'admire',\"delight\",\"yummy\",\n",
        "                            'confidenc', 'good', 'satisf', 'pleasant',\n",
        "                            'proud', 'amus', 'amazing', 'awesome',\"love\",\"passion\",\"great\",\"like\",\"wow\",\"delicious\", \"true\", \"correct\", \"crazy\"]\n",
        "            for pos in positivewords:\n",
        "                if pos in tokens:\n",
        "                    count += 1\n",
        "            return count\n",
        "\n",
        "        def capitalratio(tweet_text):\n",
        "            uppers = [l for l in tweet_text if l.isupper()]\n",
        "            capitalratio = len(uppers) / len(tweet_text)\n",
        "            return capitalratio\n",
        "\n",
        "        def contentlength(words):\n",
        "            wordcount = len(words)\n",
        "            return wordcount\n",
        "\n",
        "        def sentimentscore(tweet_text):\n",
        "            analysis = TextBlob(tweet_text)\n",
        "            return analysis.sentiment.polarity\n",
        "\n",
        "        def getposcount(tweet_text):\n",
        "            postag = []\n",
        "            poscount = {}\n",
        "            poscount['Noun']=0\n",
        "            poscount['Verb']=0\n",
        "            poscount['Adjective'] = 0\n",
        "            poscount['Pronoun']=0\n",
        "            poscount['FirstPersonPronoun']=0\n",
        "            poscount['SecondPersonPronoun']=0\n",
        "            poscount['ThirdPersonPronoun']=0\n",
        "            poscount['Adverb']=0\n",
        "            Nouns = {'NN','NNS','NNP','NNPS'}\n",
        "            Verbs={'VB','VBP','VBZ','VBN','VBG','VBD','To'}\n",
        "            first_person_pronouns=['I','me','my','mine','we','us','our','ours']\n",
        "            second_person_pronouns=['you','your','yours']\n",
        "            third_person_pronouns=['he','she','it','him','her','it','his','hers','its','they','them','their','theirs']\n",
        "\n",
        "            word_tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '', tweet_text))\n",
        "            for word in word_tokens:\n",
        "                w_lower=word.lower()\n",
        "                if w_lower in first_person_pronouns:\n",
        "                    poscount['FirstPersonPronoun']+=1\n",
        "                elif w_lower in second_person_pronouns:\n",
        "                    poscount['SecondPersonPronoun']+=1\n",
        "                elif w_lower in third_person_pronouns:\n",
        "                    poscount['ThirdPersonPronoun']+=1\n",
        "\n",
        "            postag = nltk.pos_tag(word_tokens)\n",
        "            for g1 in postag:\n",
        "                if g1[1] in Nouns:\n",
        "                    poscount['Noun'] += 1\n",
        "                elif g1[1] in Verbs:\n",
        "                    poscount['Verb']+= 1\n",
        "                elif g1[1]=='ADJ'or g1[1]=='JJ':\n",
        "                    poscount['Adjective']+=1\n",
        "                elif g1[1]=='PRP' or g1[1]=='PRON':\n",
        "                    poscount['Pronoun']+=1\n",
        "                elif g1[1]=='ADV':\n",
        "                    poscount['Adverb']+=1\n",
        "            return poscount\n",
        "        def tweets2tokens(tweet_text):\n",
        "            tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', tweet_text.lower()))\n",
        "            url=0\n",
        "            for token in tokens:\n",
        "                if token.startswith( 'http' ):\n",
        "                    url=1\n",
        "\n",
        "            return tokens,url\n",
        "\n",
        "\n",
        "        # the code for def tweettext2features(tweet_text):\n",
        "        features=dict()\n",
        "\n",
        "        tokens,url=tweets2tokens(tweet_text)\n",
        "\n",
        "        punc_dict=punctuationanalysis(tweet_text)\n",
        "        features.update(punc_dict)\n",
        "        features['negativewordcount']=(negativewordcount(tokens))\n",
        "        features['positivewordcount']=(positivewordcount(tokens))\n",
        "        features['capitalratio']=(capitalratio(tweet_text))\n",
        "        features['contentlength']=(contentlength(tokens))\n",
        "        features['sentimentscore']=(sentimentscore(tweet_text))\n",
        "        pos_dict=getposcount(tweet_text)\n",
        "        features.update(pos_dict)\n",
        "        features['has_url_in_text']=(url)\n",
        "        #print(\"features\",features)\n",
        "        return features\n",
        "\n",
        "    def export(self):\n",
        "        fn = \"%s/%s.csv\" % (self.output_dir, self.event)\n",
        "        df = pd.DataFrame(data=self.data)\n",
        "        df.to_csv(fn, index=False)\n",
        "        return fn\n",
        "    \n",
        "    def datestr_to_tmsp(self, datestr):\n",
        "        return to_unix_tmsp([parse_twitter_datetime(datestr)])[0]\n",
        "\n",
        "def pheme_to_csv(event, Parser=Tweets, output=\"/content\"):\n",
        "    start = time.time()\n",
        "    data = Parser(event, output_dir=output)\n",
        "    dataset = \"/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Data/pheme-rnr-dataset\"\n",
        "    thread_number = 0         \n",
        "    for category in os.listdir(\"%s/%s\" % (dataset, event)):\n",
        "        print('event:',event,'category:',category,category=='rumours')\n",
        "        for thread in os.listdir(\"%s/%s/%s\" % (dataset, event, category)):\n",
        "            with open(\"%s/%s/%s/%s/source-tweet/%s.json\" % (dataset, event, category, thread, thread)) as f:\n",
        "                tweet = json.load(f)\n",
        "            data.append(tweet, category, thread, True)\n",
        "            thread_number += 1\n",
        "            for reaction in os.listdir(\"%s/%s/%s/%s/reactions\" % (dataset, event, category, thread)):\n",
        "                with open(\"%s/%s/%s/%s/reactions/%s\" % (dataset, event, category, thread, reaction)) as f:\n",
        "                    tweet = json.load(f)\n",
        "                data.append(tweet, category, thread, False)\n",
        "    fn = data.export()\n",
        "    print(\"%s was generated in %s minutes\" % (fn, (time.time() - start) / 60))\n",
        "    return None\n",
        "\n",
        "def agg_event_data(df, limit=0):\n",
        "    data = df.head(limit) if limit > 0 else df\n",
        "    data = data.replace({\"has_url\": {\"True\": True, \"False\": False}})\n",
        "    agg = data.groupby(\"thread\") \\\n",
        "        .agg({\"favorite_count\": sum,\n",
        "              \"retweet_count\": sum,\n",
        "              \"is_rumor\": max,\n",
        "              \"has_url\": lambda col: np.count_nonzero(col) / len(col),\n",
        "              \"id\": len,\n",
        "              \"hashtags_count\": lambda col: len([True for total in col if total > 0]) / len(col),\n",
        "              \"text\": lambda col: len([True for txt in col if \"😊\" in txt]) / len(col)}) \\\n",
        "        .rename(columns={\"favorite_count\": \"favorite_total\",\n",
        "                         \"retweet_count\": \"retweet_total\",\n",
        "                         \"user.friends_count\": \"friends_total\",\n",
        "                         \"id\": \"thread_length\",\n",
        "                         \"has_url\":\"url_proportion\",\n",
        "                         \"hashtags_count\": \"hashtag_proportion\",\n",
        "                         \"text\": \"smile_emoji_proportion\"})\n",
        "    src = data[data[\"thread\"] == data[\"id\"]][[\"thread\", \"user.followers_count\"]]  # source tweets will have equal thread id and tweet id\n",
        "    src = src.rename(columns={\"user.followers_count\": \"src_followers_count\"})\n",
        "    thrd_data = pd.merge(agg, src, on=\"thread\")\n",
        "    return thrd_data\n",
        "\n",
        "\n",
        "events=[\n",
        "            \"germanwings-crash\"\n",
        "            #\"sydneysiege\",\n",
        "            #\"ottawashooting\"\n",
        "            #\"ferguson\",\n",
        "            #\"charliehebdo\",\n",
        "        ]\n",
        "dataset = \"/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Data/pheme-rnr-dataset\"\n",
        "processes=[]\n",
        "\n",
        "            "
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReSBjbuPl-YD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f1177a2f-8093-4a7b-b279-6954b74fcf53"
      },
      "source": [
        "'''for event in events:\n",
        "   p=Process(target=pheme_to_csv,args=(event,))\n",
        "   p.start()\n",
        "   processes.append(p)\n",
        "   for p in processes:\n",
        "     p.join()'''"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for event in events:\\n   p=Process(target=pheme_to_csv,args=(event,))\\n   p.start()\\n   processes.append(p)\\n   for p in processes:\\n     p.join()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y80vv-LDaWL7"
      },
      "source": [
        "### Combined model for content veracity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEHRnFKaaVi_",
        "outputId": "ec1b604c-d820-483f-cf3e-321d83244a47"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import nltk\n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pandas_profiling import ProfileReport\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, precision_recall_curve, recall_score, precision_score, precision_recall_curve, roc_curve, auc\n",
        "def muller_loop_classifiers(x, y):\n",
        "    classifiers = [\n",
        "        (\"Nearest Neighbors\", KNeighborsClassifier(2)),\n",
        "        (\"Linear SVM\", SVC(kernel=\"linear\", C=0.025)),\n",
        "        (\"RBF SVM\", SVC(gamma=2, C=1)),\n",
        "        (\"Decision Tree\", DecisionTreeClassifier(max_depth=5)),\n",
        "        (\"Random Forest\", RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
        "        (\"Neural Net\", MLPClassifier(alpha=1, max_iter=1000)),\n",
        "        (\"AdaBoost\", AdaBoostClassifier()),\n",
        "        (\"Naive Bayes\", GaussianNB()),\n",
        "        (\"QDA\", QuadraticDiscriminantAnalysis())\n",
        "    ]\n",
        "    #print(x.shape)\n",
        "    x = StandardScaler().fit_transform(x)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "    best_name = None\n",
        "    best_score = 0.0\n",
        "    best_classifier = None\n",
        "    plot_x = []\n",
        "    plot_y = []\n",
        "    index = 0\n",
        "    result = PrettyTable()\n",
        "    result.field_names = [\"Model\", \"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"]\n",
        "    #w2v_model = gensim.models.Word2Vec(sentences=X, size=300, window=5, min_count=1)\n",
        "    #w2v_model.train(x_train,total_examples=w2v_model.corpus_count,epochs=10)\n",
        "    #vectorizer=TfidfVectorizer(max_features=10000, stop_words=stopwords.words('german'), ngram_range=(1, 3))\n",
        "    for name, classifier in classifiers:\n",
        "        start_time = time()\n",
        "        #p = Pipeline([\n",
        "        #('classifier_CV',vectorizer),\n",
        "        #(name,classifier)])\n",
        "        classifier.fit(x_train, y_train)\n",
        "        score = classifier.score(x_test, y_test) * 100\n",
        "        predictions = classifier.predict(x_test)\n",
        "        f1Score = f1_score(y_test, predictions, average='macro')\n",
        "        precision = precision_score(y_test,predictions,average='macro')\n",
        "        recall = recall_score(y_test, predictions, average='macro')\n",
        "        print('Classifier = %s, Score (test, accuracy) = %.2f,' %(name, score), 'Training time = %.2f seconds' % (time() - start_time))\n",
        "        result.add_row([name, score, f1Score, precision, recall])\n",
        "        #print(\"Classifier = %s, Score = %.2f, Training Time = %.2f\" % (\n",
        "         #   name,  score,time() - start_time),\"f1_score: \", F1_score )\n",
        "        if score > best_score:\n",
        "            best_name = name\n",
        "            best_score = score\n",
        "            best_classifier = classifier\n",
        "        plot_x.append(name)\n",
        "        plot_y.append(score)\n",
        "        index += 1\n",
        "        path='/content/'+name+'.pkl'\n",
        "        #print(path)\n",
        "        with open(path, 'wb') as file:  \n",
        "                    pickle.dump(classifier, file)\n",
        "\n",
        "    print(100 * \"-\")\n",
        "    print(\"Best --> Classifier = %s, Score = %.2f\" % (best_name, best_score))\n",
        "\n",
        "    plt.bar(plot_x, plot_y)\n",
        "    plt.xticks(rotation=75)\n",
        "    plt.show()\n",
        "\n",
        "    return best_classifier"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AWHnxFfe7sG"
      },
      "source": [
        "### Scrape twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlz2lsIQe_BA"
      },
      "source": [
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "import json\n",
        "\n",
        "import re\n",
        " \n",
        "\n",
        "consumer_key ='iz8iwNx5S829k1HHZNiKW7FjR'\n",
        "consumer_secret = 'MBm1XBlcMuKiZNpaow3n6pD6JlsebhSUFymBwPK4IrNWumbeKQ'\n",
        "access_token = \"1308414905888972800-nNxvZKdxWvik4sCIDY7xxpTkmnxygd\"\n",
        "access_secret = \"3hDFrHeB4xNTjOwq4OYdps4RBJQOiYftDxuAKImo6bVcD\"\n",
        "\n",
        " \n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_secret)\n",
        " \n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S1kEvIbfC_x"
      },
      "source": [
        "from tweepy import Stream\n",
        "from tweepy.streaming import StreamListener\n",
        "import csv\n",
        "class MyListener(StreamListener):\n",
        "    count=0\n",
        "\n",
        "    def on_data(self, data):\n",
        "        try:\n",
        "            if MyListener.count>5:\n",
        "               return False\n",
        "            MyListener.count=MyListener.count+1\n",
        "            with open('python.json', 'a') as f:\n",
        "                f.write(data)\n",
        "                return True\n",
        "        except BaseException as e:\n",
        "            print(\"Error on_data: %s\" % str(e))\n",
        "        return True\n",
        " \n",
        "    def on_error(self, status):\n",
        "        print(status)\n",
        "        return True\n",
        " \n",
        "twitter_stream = Stream(auth, MyListener())\n",
        "tweets=twitter_stream.filter(track=['#trending'])"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn070QDBYjJg"
      },
      "source": [
        "### Streaming data features extraction and prediction\n",
        "\n",
        "python.json is scraped twitter data using tweepy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIzg0YJ89i2k"
      },
      "source": [
        "def contentvearcity_score(d):\n",
        "  temp = d\n",
        "  d=d[['tweet_length','symbol_count','urls_count',\n",
        "                 'media_count','has_smile_emoji','sensitive', 'hashtags_count', 'retweet_count','has_place', 'has_coords',\n",
        "                 'has_quest_or_exclaim','number_punct','negativewordcount',\n",
        "                 'positivewordcount', 'capitalratio','contentlength', 'sentimentscore', \n",
        "                 'Noun', 'Verb', 'Adjective','Pronoun', 'FirstPersonPronoun', \n",
        "                 'SecondPersonPronoun','ThirdPersonPronoun', 'Adverb']]\n",
        "  #d=d.dropna()    \n",
        "  # print(d.shape)           \n",
        "  load_rf=pickle.load(open('/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Rishitha/week9/data and models/Random Forest.pkl','rb'))\n",
        "\n",
        "  predict_rf=load_rf.predict(d)\n",
        "  scores_rf = load_rf.predict_proba(d)\n",
        "  # print(predict_rf)\n",
        "  # print(scores_rf[:,0])\n",
        "\n",
        "  d.loc[:,'contentVeracityScore'] = scores_rf[:,0]\n",
        "  d.loc[:,'tweet_id'] = temp['tweet_id']\n",
        "  # d.loc[:,'tweet'] = temp['tweet']\n",
        "  d = d[['tweet_id','contentVeracityScore']]\n",
        "\n",
        "  return d"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TF3JANmYWvm"
      },
      "source": [
        "import pandas as pd\n",
        "def extract_features_stream():\n",
        "  df = pd.read_json (r'/content/python.json',lines=True)\n",
        "  df.to_csv (r'/content/tweet.csv', index = None)\n",
        "  df=pd.read_csv('/content/tweet.csv')\n",
        "  #print(df.columns)\n",
        "  d=pd.DataFrame(columns=['tweet_id','tweet','tweet_length','symbol_count','urls_count','media_count','retweet_count','favorite_count','hashtags_count','user_mentions','has_smile_emoji','has_place','has_coords','has_quest','has_exclaim','has_quest_or_exclaim','sensitive','hasperiod','number_punct', 'negativewordcount', 'positivewordcount', 'capitalratio', 'contentlength', 'sentimentscore', 'Noun', 'Verb', 'Adjective', 'Pronoun', 'FirstPersonPronoun' ,'SecondPersonPronoun' ,'ThirdPersonPronoun', 'Adverb'])\n",
        "  for i in df.index:\n",
        "    has_question = \"?\" in df[\"text\"][i]\n",
        "    has_exclaim = \"!\" in df[\"text\"][i]\n",
        "    has_quest_or_exclaim=(has_question or has_exclaim)\n",
        "    #print(df.entities[i])\n",
        "    df.entities[i] = df.entities[i].replace(\"\\'\", \"\\\"\")\n",
        "    en=json.loads(df.entities[i])\n",
        "    #print(en['user_mentions'])\n",
        "    features={\n",
        "        'tweet_id': df['id'][i],\n",
        "        'tweet': df['text'][i],\n",
        "        'tweet_length':len(df['text'][i]),\n",
        "        'symbol_count':len(en['symbols']),\n",
        "        'urls_count':len(en['urls']),\n",
        "        'retweet_count':df['retweet_count'][i],\n",
        "        'favorite_count':df['favorite_count'][i],\n",
        "        'media_count': len(en['media']) if 'media' in en else 0,\n",
        "        'user_mentions':len(en['user_mentions']),\n",
        "        'hashtags_count':len(en['hashtags']),\n",
        "        'has_smile_emoji':1 if \"😊\" in df[\"text\"][i] else 0,\n",
        "        'has_place':1 if df[\"place\"][i] else 0,\n",
        "        'has_coords':1 if df[\"coordinates\"][i] else 0,\n",
        "        'has_quest':has_question,\n",
        "        'has_exclaim':has_exclaim,\n",
        "        'has_quest_or_exclaim':has_quest_or_exclaim,\n",
        "        'sensitive':1 if df[\"possibly_sensitive\"][i] else 0,\n",
        "    }\n",
        "    text_features=Tweets.tweettext2features(tweets,df['text'][i])\n",
        "    #print(text_features)\n",
        "    for col in features:\n",
        "            d.loc[i,col]=features[col]\n",
        "    \n",
        "    for col in text_features:\n",
        "      #print(col,text_features[col])\n",
        "      d.loc[i,col]=text_features[col]\n",
        "\n",
        "    \n",
        "  d.to_csv('stream_features.csv', sep='\\t')\n",
        "  #contentvearcity_score(d)\n",
        "  \n",
        "  return d"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiUch6QVfbm_"
      },
      "source": [
        "d = extract_features_stream()"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "2JH44Uh5pOA8",
        "outputId": "9cbbe995-b63b-4da6-93f5-f00c4f58da7c"
      },
      "source": [
        "d.head()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_length</th>\n",
              "      <th>symbol_count</th>\n",
              "      <th>urls_count</th>\n",
              "      <th>media_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>hashtags_count</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>has_smile_emoji</th>\n",
              "      <th>has_place</th>\n",
              "      <th>has_coords</th>\n",
              "      <th>has_quest</th>\n",
              "      <th>has_exclaim</th>\n",
              "      <th>has_quest_or_exclaim</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>hasperiod</th>\n",
              "      <th>number_punct</th>\n",
              "      <th>negativewordcount</th>\n",
              "      <th>positivewordcount</th>\n",
              "      <th>capitalratio</th>\n",
              "      <th>contentlength</th>\n",
              "      <th>sentimentscore</th>\n",
              "      <th>Noun</th>\n",
              "      <th>Verb</th>\n",
              "      <th>Adjective</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>FirstPersonPronoun</th>\n",
              "      <th>SecondPersonPronoun</th>\n",
              "      <th>ThirdPersonPronoun</th>\n",
              "      <th>Adverb</th>\n",
              "      <th>has_url_in_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1389685252608565249</td>\n",
              "      <td>RT @marissa_account: HAPPY CHILDREN’S DAY TXT!...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.485714</td>\n",
              "      <td>15</td>\n",
              "      <td>0.8</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1389685346061983747</td>\n",
              "      <td>RT @OFFTV8: I Am NorthEast - Grew Up - https:/...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.235714</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1389685389766533120</td>\n",
              "      <td>RT @goddessjaydebbg: I want you to give me 1 g...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0357143</td>\n",
              "      <td>28</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1389685436600233986</td>\n",
              "      <td>Beliebt in 🇨🇳 China: OS Spann- und Befestigung...</td>\n",
              "      <td>127</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.133858</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1389685437825003522</td>\n",
              "      <td>#NP Nipsey Hussle Through My Eyes (feat. Drake...</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>16</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  ... has_url_in_text\n",
              "0  1389685252608565249  ...             0.0\n",
              "1  1389685346061983747  ...             1.0\n",
              "2  1389685389766533120  ...             0.0\n",
              "3  1389685436600233986  ...             1.0\n",
              "4  1389685437825003522  ...             1.0\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGe8g4wCjsAo"
      },
      "source": [
        "## Inference dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9MAx_S73n7c"
      },
      "source": [
        "def model_label(x):\n",
        "  if x > 0.80:\n",
        "      return 'True'\n",
        "  elif x > 0.70:\n",
        "      return 'Mostly true'\n",
        "  elif x > 0.40:\n",
        "      return 'Half true'\n",
        "  elif x > 0.30:\n",
        "      return 'Mostly fasle'\n",
        "  elif x > 0.20:\n",
        "      return 'false'\n",
        "  else:\n",
        "      return 'Pants-on-fire'"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVqmp1tBdBiV"
      },
      "source": [
        "#combining social credibility and content veracity micro factors\n",
        "def getTweetType(d):\n",
        "    content_veracity_dataset = contentvearcity_score(d)\n",
        "    social_credibility_dataset = getSocialScore(d, 'tweet')\n",
        "    social_credibility_dataset = pd.merge(social_credibility_dataset, content_veracity_dataset, left_on='tweet_id', right_on='tweet_id', how='left')\n",
        "\n",
        "    #accuracy of social credibility model,content veracity model\n",
        "    accuracy = [0.5624, 0.8756]\n",
        "\n",
        "    w = [float(i)/sum(accuracy) for i in accuracy]\n",
        "    print(w)\n",
        "\n",
        "    social_credibility_dataset['model'] = social_credibility_dataset['social_credibility']*w[0] + social_credibility_dataset['contentVeracityScore']*w[1]\n",
        "    social_credibility_dataset['type'] = social_credibility_dataset[\"model\"].apply(model_label) \n",
        "    social_credibility_dataset = social_credibility_dataset.drop('contentVeracityScore',1)\n",
        "    social_credibility_dataset = social_credibility_dataset.drop('social_credibility',1)\n",
        "    print(social_credibility_dataset.head())\n",
        "    return social_credibility_dataset"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xbuMotnJmaS"
      },
      "source": [
        "social_content = getTweetType(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1R4R3yeQ4nrP",
        "outputId": "15505e3a-198a-4a92-ec37-edd406a7653b"
      },
      "source": [
        "social_content.head()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>model</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1389685252608565249</td>\n",
              "      <td>RT @marissa_account: HAPPY CHILDREN’S DAY TXT!...</td>\n",
              "      <td>0.507421</td>\n",
              "      <td>Half true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1389685346061983747</td>\n",
              "      <td>RT @OFFTV8: I Am NorthEast - Grew Up - https:/...</td>\n",
              "      <td>0.521742</td>\n",
              "      <td>Half true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1389685389766533120</td>\n",
              "      <td>RT @goddessjaydebbg: I want you to give me 1 g...</td>\n",
              "      <td>0.540282</td>\n",
              "      <td>Half true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1389685436600233986</td>\n",
              "      <td>Beliebt in 🇨🇳 China: OS Spann- und Befestigung...</td>\n",
              "      <td>0.442691</td>\n",
              "      <td>Half true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1389685437825003522</td>\n",
              "      <td>#NP Nipsey Hussle Through My Eyes (feat. Drake...</td>\n",
              "      <td>0.518275</td>\n",
              "      <td>Half true</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  ...       type\n",
              "0  1389685252608565249  ...  Half true\n",
              "1  1389685346061983747  ...  Half true\n",
              "2  1389685389766533120  ...  Half true\n",
              "3  1389685436600233986  ...  Half true\n",
              "4  1389685437825003522  ...  Half true\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmkev81ZeoGt"
      },
      "source": [
        "# News Coverage - Tripura\n",
        "Micro factors\n",
        "* Similar News\n",
        "* News reach\n",
        "* Sentiment analysis\n",
        "\n",
        "Used the scrapped dataset above and applied my microfactors on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miXUJHlAk4Ud"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from string import punctuation\n",
        "import collections\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import en_core_web_sm\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import jaccard_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "ydnKq2XRe3Fp",
        "outputId": "d8322adf-bcf2-4845-b659-8ca551784891"
      },
      "source": [
        "d.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_length</th>\n",
              "      <th>symbol_count</th>\n",
              "      <th>urls_count</th>\n",
              "      <th>media_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>hashtags_count</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>has_smile_emoji</th>\n",
              "      <th>has_place</th>\n",
              "      <th>has_coords</th>\n",
              "      <th>has_quest</th>\n",
              "      <th>has_exclaim</th>\n",
              "      <th>has_quest_or_exclaim</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>hasperiod</th>\n",
              "      <th>number_punct</th>\n",
              "      <th>negativewordcount</th>\n",
              "      <th>positivewordcount</th>\n",
              "      <th>capitalratio</th>\n",
              "      <th>contentlength</th>\n",
              "      <th>sentimentscore</th>\n",
              "      <th>Noun</th>\n",
              "      <th>Verb</th>\n",
              "      <th>Adjective</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>FirstPersonPronoun</th>\n",
              "      <th>SecondPersonPronoun</th>\n",
              "      <th>ThirdPersonPronoun</th>\n",
              "      <th>Adverb</th>\n",
              "      <th>has_url_in_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1389644760890097667</td>\n",
              "      <td>Fashion Queen Forever👸 🍭😹💜\\n\\n#fyp #foryoupage...</td>\n",
              "      <td>134</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0820896</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1389644791210672130</td>\n",
              "      <td>RT @ragspedia: [HOMEMADE] Lemon sugar cookies!...</td>\n",
              "      <td>139</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0863309</td>\n",
              "      <td>15</td>\n",
              "      <td>-0.9375</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1389644829462781961</td>\n",
              "      <td>RT @mi_mitic: Vacation mood ~ still on #gym #s...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0357143</td>\n",
              "      <td>17</td>\n",
              "      <td>0.45</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1389644842037391367</td>\n",
              "      <td>REAL #GERMOLENE #PINK #OINTMENT\\n#New Tubes At...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>14</td>\n",
              "      <td>0.0787879</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1389644851008974856</td>\n",
              "      <td>پاکستان کے اسسٹنٹ کمشنر کی عیاشیاں اور فرعونیت...</td>\n",
              "      <td>123</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0731707</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  ... has_url_in_text\n",
              "0  1389644760890097667  ...             1.0\n",
              "1  1389644791210672130  ...             1.0\n",
              "2  1389644829462781961  ...             1.0\n",
              "3  1389644842037391367  ...             1.0\n",
              "4  1389644851008974856  ...             1.0\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzscODFoYe3"
      },
      "source": [
        "def wordcloud(data):\n",
        "    plt.figure(figsize = (10, 10))\n",
        "    wordcloud = WordCloud().generate(data)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    \n",
        "wordcloud(' '.join(d['tweet']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmsY5d9QlOTc"
      },
      "source": [
        "# Preprocessing\n",
        "Tweets Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "WGrmZC44k8a8",
        "outputId": "c99f5b55-d303-4f87-c182-68cefbdedbcf"
      },
      "source": [
        "# remove the hashtags, mentions and unwanted characters.\n",
        "def clean_text(d, text_field):\n",
        "    d[text_field] = d[text_field].str.lower()\n",
        "    d[text_field] = d[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0–9]+)|([0-9A-Za-z \\t])\", \"\",   elem)) \n",
        "    return d\n",
        "d = clean_text(d, 'tweet')\n",
        "d.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_length</th>\n",
              "      <th>symbol_count</th>\n",
              "      <th>urls_count</th>\n",
              "      <th>media_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>hashtags_count</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>has_smile_emoji</th>\n",
              "      <th>has_place</th>\n",
              "      <th>has_coords</th>\n",
              "      <th>has_quest</th>\n",
              "      <th>has_exclaim</th>\n",
              "      <th>has_quest_or_exclaim</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>hasperiod</th>\n",
              "      <th>number_punct</th>\n",
              "      <th>negativewordcount</th>\n",
              "      <th>positivewordcount</th>\n",
              "      <th>capitalratio</th>\n",
              "      <th>contentlength</th>\n",
              "      <th>sentimentscore</th>\n",
              "      <th>Noun</th>\n",
              "      <th>Verb</th>\n",
              "      <th>Adjective</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>FirstPersonPronoun</th>\n",
              "      <th>SecondPersonPronoun</th>\n",
              "      <th>ThirdPersonPronoun</th>\n",
              "      <th>Adverb</th>\n",
              "      <th>has_url_in_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1389644760890097667</td>\n",
              "      <td>👸🍭😹💜#######…://./</td>\n",
              "      <td>134</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0820896</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1389644791210672130</td>\n",
              "      <td>:[]!########://./</td>\n",
              "      <td>139</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0863309</td>\n",
              "      <td>15</td>\n",
              "      <td>-0.9375</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1389644829462781961</td>\n",
              "      <td>_:~##########://./…</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0357143</td>\n",
              "      <td>17</td>\n",
              "      <td>0.45</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1389644842037391367</td>\n",
              "      <td>###://./##_##…://./</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>14</td>\n",
              "      <td>0.0787879</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1389644851008974856</td>\n",
              "      <td></td>\n",
              "      <td>123</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0731707</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id                tweet  ... Adverb has_url_in_text\n",
              "0  1389644760890097667    👸🍭😹💜#######…://./  ...      0             1.0\n",
              "1  1389644791210672130    :[]!########://./  ...      0             1.0\n",
              "2  1389644829462781961  _:~##########://./…  ...      0             1.0\n",
              "3  1389644842037391367  ###://./##_##…://./  ...      0             1.0\n",
              "4  1389644851008974856                       ...      0             1.0\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck8vPBaGlSOk"
      },
      "source": [
        "# Tokenization, Lemmatization and removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctsifBiBlQuR"
      },
      "source": [
        "nlp = en_core_web_sm.load() \n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation) #already taken care of with the cleaning function.\n",
        "stop.update(punctuation)\n",
        "w_tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "\n",
        "def furnished(text):\n",
        "    final_text = []\n",
        "    for i in w_tokenizer.tokenize(text):\n",
        "       if i.lower() not in stop:\n",
        "        word = lemmatizer.lemmatize(i)\n",
        "        final_text.append(word.lower())\n",
        "    return \" \".join(final_text)\n",
        "d.tweet = d.tweet.apply(furnished)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "7W_aWTmMnkUg",
        "outputId": "9bf56f24-ad9e-4578-8414-b2311389629d"
      },
      "source": [
        "d.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_length</th>\n",
              "      <th>symbol_count</th>\n",
              "      <th>urls_count</th>\n",
              "      <th>media_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>hashtags_count</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>has_smile_emoji</th>\n",
              "      <th>has_place</th>\n",
              "      <th>has_coords</th>\n",
              "      <th>has_quest</th>\n",
              "      <th>has_exclaim</th>\n",
              "      <th>has_quest_or_exclaim</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>hasperiod</th>\n",
              "      <th>number_punct</th>\n",
              "      <th>negativewordcount</th>\n",
              "      <th>positivewordcount</th>\n",
              "      <th>capitalratio</th>\n",
              "      <th>contentlength</th>\n",
              "      <th>sentimentscore</th>\n",
              "      <th>Noun</th>\n",
              "      <th>Verb</th>\n",
              "      <th>Adjective</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>FirstPersonPronoun</th>\n",
              "      <th>SecondPersonPronoun</th>\n",
              "      <th>ThirdPersonPronoun</th>\n",
              "      <th>Adverb</th>\n",
              "      <th>has_url_in_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1389644760890097667</td>\n",
              "      <td>fashion queen forever👸 🍭😹💜 #fyp #foryoupage #t...</td>\n",
              "      <td>134</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0820896</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1389644791210672130</td>\n",
              "      <td>rt @ragspedia: [homemade] lemon sugar cookies!...</td>\n",
              "      <td>139</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0863309</td>\n",
              "      <td>15</td>\n",
              "      <td>-0.9375</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1389644829462781961</td>\n",
              "      <td>rt @mi_mitic: vacation mood still #gym #sexy #...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0357143</td>\n",
              "      <td>17</td>\n",
              "      <td>0.45</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1389644842037391367</td>\n",
              "      <td>real #germolene #pink #ointment #new tubes htt...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>14</td>\n",
              "      <td>0.0787879</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1389644851008974856</td>\n",
              "      <td>پاکستان کے اسسٹنٹ کمشنر کی عیاشیاں اور فرعونیت...</td>\n",
              "      <td>123</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0731707</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  ... has_url_in_text\n",
              "0  1389644760890097667  ...             1.0\n",
              "1  1389644791210672130  ...             1.0\n",
              "2  1389644829462781961  ...             1.0\n",
              "3  1389644842037391367  ...             1.0\n",
              "4  1389644851008974856  ...             1.0\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5ekuM9-pv6K"
      },
      "source": [
        "economy_related_words = '''agriculture infrastructure capitalism trading service sector technology  economical supply \n",
        "                          industrialism efficiency frugality retrenchment downsizing   credit debit value \n",
        "                         economize   save  economically\n",
        "                         economies sluggish rise   rising spending conserve trend \n",
        "                         low-management  decline   industry impact poor  \n",
        "                            profession    surplus   fall\n",
        "                         declining  accelerating interest sectors balance stability productivity increase rates\n",
        "                            pushing expanding stabilize  rate industrial borrowing struggling\n",
        "                           deficit predicted    increasing  data\n",
        "                          economizer analysts investment market-based economy   debt free enterprise\n",
        "                         medium  exchange metric savepoint scarcity capital bank company stockholder fund business  \n",
        "                         asset treasury tourism incomes contraction employment jobs upturn deflation  macroeconomics\n",
        "                         bankruptcies exporters hyperinflation dollar entrepreneurship upswing marketplace commerce devaluation \n",
        "                         quicksave deindustrialization stockmarket reflation downspin dollarization withholder bankroll venture capital\n",
        "                         mutual fund plan economy mortgage lender unemployment rate credit crunch central bank financial institution\n",
        "                         bank rate custom duties mass-production black-market developing-countries developing economic-growth gdp trade barter \n",
        "                         distribution downturn economist'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouUvgM0dqY8O"
      },
      "source": [
        "social_related_words = '''sociable, gregarious societal friendly society socialization political  sociality \n",
        "                        interpersonal  ethnic socially party welfare public community socialist societies development\n",
        "                            network humans socialism collective personal corporation social constructivism\n",
        "                        relations volition citizenship brute   attitude rights socio \n",
        "                        socioeconomic ethics civic communal marital  sociale socialized communities     \n",
        "                         policy   unions        \n",
        "                        institutions values     governmental   organizations jamboree \n",
        "                         festivity    fairness  support  care  \n",
        "                         sides   activism     unsocial psychosocial \n",
        "                        socializing psychological distributional  demographic  participation reunion \n",
        "                        partygoer partyism festive power network gala housewarming celebration counterparty   social-war\n",
        "                        particularist interactional ideational asocial'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I82ls_Gqqbbw"
      },
      "source": [
        "culture_related_words  = ''' ethnicity heritage modernity spirituality marxismmaterial culture \n",
        "                           ethos nationality humanism romanticism civilisation traditionalism genetics\n",
        "                        kinship heredity marriage   indigenous  archeology  acculturate  \n",
        "                       ontogenesis viniculture modern clothes     rooted \n",
        "                       cicero societies history roots influence geography historical folk origins \n",
        "                       phenomenon teleology ancient aspects perspective liberalism nowadays community style unique prevalent describes \n",
        "                         today  origin   modernity beliefs  genre barbarian ethnic \n",
        "                       colonization cultural universal organization western-civilization structuralism  culture \n",
        "                       heathen pagan transculturation culture peasant classicist nativism anarchy ungrown philosophic cult  \n",
        "                       consciousness islamist bro-culture evolve cultic diaspora aftergrowth native cultural-relativism  \n",
        "                       mongolian cosmopolitan epistemology lifestyles diversity chauvinism westernization materialism vernacular \n",
        "                       homogeneity otherness holism tusculanae disputationes primitivism superficiality hedonism discourse\n",
        "                       puritanism modernism intellectualism  exclusiveness elitism  colonialism  \n",
        "                       pentecostalism paganism nationwide expansion rural  auxesis kimono \n",
        "                       culturize alethophobia nettlebed japanification  dongyi clannishness insularity hybridity\n",
        "                       westernisation foreignness worldview exclusionism enculturation ethnocentrism  confucianist vulgarization\n",
        "                       shintoism  westernism denominationalism    deracination\n",
        "                        eurocentrism  cosmologies  emotiveness bohemianism territorialism\n",
        "                       philosophical-doctrine ethnic minority social-darwinism  theory cultural evolution belief systemfolk music \n",
        "                       traditional art house karl-marx   theorymedia  \n",
        "                       film-theory art history museum studies cultural artifact'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKlWQialqd_R"
      },
      "source": [
        "health_related_words = '''disease obesity world health organization medicine nutrition well-being exercise welfare wellness health care public health \n",
        "                     nursing stress safety hygiene research social healthy condition aids epidemiology healthiness wellbeing\n",
        "                     care illness medical dieteducation infectious disease environmental healthcare physical fitness hospitals \n",
        "                     health care provider doctors healthy community design insurance sanitation human body patient mental health\n",
        "                      medicare agriculture health science fitnesshealth policy  weight loss physical therapy psychology pharmacy\n",
        "                     metabolic organism human lifestyle status unhealthy upbeat vaccination sleep condom alcohol smoking water family\n",
        "                     eudaimonia eudaemonia air house prevention genetics public families poor needs treatment communicable disease \n",
        "                     study protection malaria development food priority management healthful mental provide department administration\n",
        "                     programs help assistance funding environment improving emergency need program affected schools private mental illness \n",
        "                     treat diseases preparedness perinatal fertility sickness veterinary sanitary pharmacists behavioral midwives\n",
        "                     gerontology infertility hospitalization midwifery cholesterol childcare pediatrician pediatrics medicaid asthma \n",
        "                     pensions sicknesses push-up physical education body-mass-index eat well gymnastic apparatus tune up good morning \n",
        "                     bathing low blood-pressure heart attack health club ride-bike you feel good eczema urticaria dermatitis sunburn overwork \n",
        "                     manufacturing medical sociology need exercise run'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz9muA1rpySx"
      },
      "source": [
        "economy = furnished(economy_related_words)\n",
        "social = furnished(social_related_words)\n",
        "culture = furnished(culture_related_words)\n",
        "health = furnished(health_related_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxwUElyqvDP"
      },
      "source": [
        "Removing Duplicated words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "SJAkHm1TqF8B",
        "outputId": "284d3a4e-71dd-49b4-be76-c093442aa41b"
      },
      "source": [
        "string1 = economy\n",
        "words = string1.split()\n",
        "economy = \" \".join(sorted(set(words), key=words.index))\n",
        "economy\n",
        "\n",
        "string1 = social\n",
        "words = string1.split()\n",
        "social = \" \".join(sorted(set(words), key=words.index))\n",
        "social\n",
        "\n",
        "string1 = health\n",
        "words = string1.split()\n",
        "health = \" \".join(sorted(set(words), key=words.index))\n",
        "health\n",
        "\n",
        "string1 = culture\n",
        "words = string1.split()\n",
        "culture = \" \".join(sorted(set(words), key=words.index))\n",
        "culture"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ethnicity heritage modernity spirituality marxismmaterial culture ethos nationality humanism romanticism civilisation traditionalism genetics kinship heredity marriage indigenous archeology acculturate ontogenesis viniculture modern clothes rooted cicero society history root influence geography historical folk origin phenomenon teleology ancient aspect perspective liberalism nowadays community style unique prevalent describes today belief genre barbarian ethnic colonization cultural universal organization western-civilization structuralism heathen pagan transculturation peasant classicist nativism anarchy ungrown philosophic cult consciousness islamist bro-culture evolve cultic diaspora aftergrowth native cultural-relativism mongolian cosmopolitan epistemology lifestyle diversity chauvinism westernization materialism vernacular homogeneity otherness holism tusculanae disputationes primitivism superficiality hedonism discourse puritanism modernism intellectualism exclusiveness elitism colonialism pentecostalism paganism nationwide expansion rural auxesis kimono culturize alethophobia nettlebed japanification dongyi clannishness insularity hybridity westernisation foreignness worldview exclusionism enculturation ethnocentrism confucianist vulgarization shintoism westernism denominationalism deracination eurocentrism cosmology emotiveness bohemianism territorialism philosophical-doctrine minority social-darwinism theory evolution systemfolk music traditional art house karl-marx theorymedia film-theory museum study artifact'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWa2ss8pq-Wo"
      },
      "source": [
        "# Vectorizing the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoxkyOfAqySp"
      },
      "source": [
        "def get_vectors(*strs):\n",
        "    text = [t for t in strs]\n",
        "    vectorizer = TfidfVectorizer(text)\n",
        "    vectorizer.fit(text)\n",
        "    return vectorizer.transform(text).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG3tDWGSq3Ww"
      },
      "source": [
        "socialvector = get_vectors(social)\n",
        "economic_vector = get_vectors(economy)\n",
        "culture_vector = get_vectors(culture)\n",
        "health_vector = get_vectors(health)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLfbL-j-q8fY"
      },
      "source": [
        "tv=TfidfVectorizer()\n",
        "# d = d.tweet.apply(get_vectors)\n",
        "# d.head()\n",
        "tfidf_tweets =tv.fit_transform(d.tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAclSFSJrXri"
      },
      "source": [
        "Similarity Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF6ICtfqrXGC"
      },
      "source": [
        "def jaccard_similarity(query, document):\n",
        "    intersection = set(query).intersection(set(document))\n",
        "    union = set(query).union(set(document))\n",
        "    return len(intersection)/len(union)\n",
        "def get_scores(group,tweets):\n",
        "    scores = []\n",
        "    for tweet in tweets:\n",
        "        s = jaccard_similarity(group, tweet)\n",
        "        scores.append(s)\n",
        "    return scores\n",
        "e_scores = get_scores(economy, d.tweet.to_list())\n",
        "s_scores = get_scores(social, d.tweet.to_list())\n",
        "c_scores = get_scores(culture, d.tweet.to_list())\n",
        "h_scores = get_scores(health, d.tweet.to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV8aO2GvrkvV"
      },
      "source": [
        "# create a jaccard scored df.\n",
        "data  = {'ID':d.tweet_id.to_list(),       'economic_score':e_scores,\n",
        "         'social_score': s_scores, 'culture_score':c_scores, 'health_scores':h_scores}\n",
        "scores_df = pd.DataFrame(data)\n",
        "#assign classes based on highest score\n",
        "def get_classes(l1, l2, l3, l4):\n",
        "    econ = []\n",
        "    socio = []\n",
        "    cul = []\n",
        "    heal = []\n",
        "    for i, j, k, l in zip(l1, l2, l3, l4):\n",
        "        m = max(i, j, k, l)\n",
        "        if m == i:\n",
        "            econ.append(1)\n",
        "        else:\n",
        "            econ.append(0)\n",
        "        if m == j:\n",
        "            socio.append(1)\n",
        "        else:\n",
        "            socio.append(0)        \n",
        "        if m == k:\n",
        "            cul.append(1)\n",
        "        else:\n",
        "            cul.append(0)  \n",
        "        if m == l:\n",
        "            heal.append(1)\n",
        "        else:\n",
        "            heal.append(0)   \n",
        "            \n",
        "    return econ, socio, cul, heal\n",
        "l1 = scores_df.economic_score.to_list()\n",
        "l2 = scores_df.social_score.to_list()\n",
        "l3 = scores_df.culture_score.to_list()\n",
        "l4 = scores_df.health_scores.to_list()\n",
        "econ, socio, cul, heal = get_classes(l1, l2, l3, l4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "445lQL3wr7PP"
      },
      "source": [
        "data = {'ID': scores_df.ID.to_list(), 'economic':econ, 'social':socio, 'culture':cul, 'health': heal}\n",
        "class_df = pd.DataFrame(data)\n",
        "#grouping the tweets by ID\n",
        "new_groups_df = class_df.groupby(['ID']).sum()\n",
        "#add a new totals column\n",
        "new_groups_df['total'] = new_groups_df['health'] + new_groups_df['culture'] + new_groups_df['social'] +  new_groups_df['economic']\n",
        "#add a new totals row\n",
        "new_groups_df.loc[\"Total\"] = new_groups_df.sum() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "VTUbkTpRtApd",
        "outputId": "6c8b03e3-1534-4e93-db38-18b457232b66"
      },
      "source": [
        "new_groups_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>economic</th>\n",
              "      <th>social</th>\n",
              "      <th>culture</th>\n",
              "      <th>health</th>\n",
              "      <th>total</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1389644760890097667</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1389644791210672130</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1389644829462781961</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1389644842037391367</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1389644851008974856</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     economic  social  culture  health  total\n",
              "ID                                                           \n",
              "1389644760890097667         1       0        1       0      2\n",
              "1389644791210672130         0       0        0       1      1\n",
              "1389644829462781961         0       0        0       1      1\n",
              "1389644842037391367         1       0        1       0      2\n",
              "1389644851008974856         1       0        1       0      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0lfvc0Nr135"
      },
      "source": [
        "fig = plt.figure(figsize =(10, 7)) \n",
        "a = new_groups_df.drop(['total'], axis = 1)\n",
        "plt.pie(a.loc['Total'], labels = a.columns)\n",
        "plt.title('A pie chart showing the volumes of tweets under different categories.')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLl4Ec8wIoT1"
      },
      "source": [
        "# Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHGW46Qso0WJ",
        "outputId": "b1991e31-032d-43a8-9d08-5a6d9ad756a7"
      },
      "source": [
        "pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 20.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 16.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDTAeOMBoruh"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "def get_text_sentiment(text):\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "  _sentiment = analyzer.polarity_scores(text)[\"compound\"]\n",
        "  return 'Positive' if abs(_sentiment) > 0.5 else 'Negative'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KonHbKLIo4Ze"
      },
      "source": [
        "d['Sentiment'] = d['tweet'].apply(get_text_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "_eP7fqJBo83H",
        "outputId": "1a9d6b83-9234-4abb-d2cb-1f4b4a9a5a41"
      },
      "source": [
        "d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_length</th>\n",
              "      <th>symbol_count</th>\n",
              "      <th>urls_count</th>\n",
              "      <th>media_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>hashtags_count</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>has_smile_emoji</th>\n",
              "      <th>has_place</th>\n",
              "      <th>has_coords</th>\n",
              "      <th>has_quest</th>\n",
              "      <th>has_exclaim</th>\n",
              "      <th>has_quest_or_exclaim</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>hasperiod</th>\n",
              "      <th>number_punct</th>\n",
              "      <th>negativewordcount</th>\n",
              "      <th>positivewordcount</th>\n",
              "      <th>capitalratio</th>\n",
              "      <th>contentlength</th>\n",
              "      <th>sentimentscore</th>\n",
              "      <th>Noun</th>\n",
              "      <th>Verb</th>\n",
              "      <th>Adjective</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>FirstPersonPronoun</th>\n",
              "      <th>SecondPersonPronoun</th>\n",
              "      <th>ThirdPersonPronoun</th>\n",
              "      <th>Adverb</th>\n",
              "      <th>has_url_in_text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1389462102264487942</td>\n",
              "      <td>RT @ChemicalCC_V2: https://t.co/LF9SJZGpoP\\nSo...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>15</td>\n",
              "      <td>0.525</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1389462108606279682</td>\n",
              "      <td>RT @viralbhayani77: The most powerful Fandom i...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0428571</td>\n",
              "      <td>21</td>\n",
              "      <td>0.5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1389462151803244548</td>\n",
              "      <td>RT @12thboardexams: #cancel12thboardexams2021 ...</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0952381</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1389462160753840130</td>\n",
              "      <td>RT @HarryHectic1: Follow Me On @Spotify HarryH...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.164286</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  ... Sentiment\n",
              "0  1389462102264487942  ...  Positive\n",
              "1  1389462108606279682  ...  Positive\n",
              "2  1389462151803244548  ...  Negative\n",
              "3  1389462160753840130  ...  Negative\n",
              "\n",
              "[4 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "fKTGt0DUo-zh",
        "outputId": "6d45e89b-0e99-4f49-ef6e-350f8949e453"
      },
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.countplot(x='Sentiment',data=d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7166761490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruxdLeZyIvVp"
      },
      "source": [
        "# News Reach\n",
        "If the No: of retweets and the hashtag counts of a particular tweet croses a threshold, then the reach is considered high else reach is low."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "75LM3bzlGA98",
        "outputId": "6ba7c558-e767-43fb-c871-4b4b9130d853"
      },
      "source": [
        "d['NewsReach'] = d['hashtags_count'].apply(lambda x: 'High' if x > 4 else 'Low')\n",
        "d.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_length</th>\n",
              "      <th>symbol_count</th>\n",
              "      <th>urls_count</th>\n",
              "      <th>media_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>hashtags_count</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>has_smile_emoji</th>\n",
              "      <th>has_place</th>\n",
              "      <th>has_coords</th>\n",
              "      <th>has_quest</th>\n",
              "      <th>has_exclaim</th>\n",
              "      <th>has_quest_or_exclaim</th>\n",
              "      <th>sensitive</th>\n",
              "      <th>hasperiod</th>\n",
              "      <th>number_punct</th>\n",
              "      <th>negativewordcount</th>\n",
              "      <th>positivewordcount</th>\n",
              "      <th>capitalratio</th>\n",
              "      <th>contentlength</th>\n",
              "      <th>sentimentscore</th>\n",
              "      <th>Noun</th>\n",
              "      <th>Verb</th>\n",
              "      <th>Adjective</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>FirstPersonPronoun</th>\n",
              "      <th>SecondPersonPronoun</th>\n",
              "      <th>ThirdPersonPronoun</th>\n",
              "      <th>Adverb</th>\n",
              "      <th>has_url_in_text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>NewsReach</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1389462102264487942</td>\n",
              "      <td>RT @ChemicalCC_V2: https://t.co/LF9SJZGpoP\\nSo...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>15</td>\n",
              "      <td>0.525</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1389462108606279682</td>\n",
              "      <td>RT @viralbhayani77: The most powerful Fandom i...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.1182</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0428571</td>\n",
              "      <td>21</td>\n",
              "      <td>0.5</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1389462151803244548</td>\n",
              "      <td>RT @12thboardexams: #cancel12thboardexams2021 ...</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0952381</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1389462160753840130</td>\n",
              "      <td>RT @HarryHectic1: Follow Me On @Spotify HarryH...</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.8227</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.164286</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Negative</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  ... NewsReach\n",
              "0  1389462102264487942  ...       Low\n",
              "1  1389462108606279682  ...       Low\n",
              "2  1389462151803244548  ...       Low\n",
              "3  1389462160753840130  ...      High\n",
              "\n",
              "[4 rows x 35 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "0YcKSnNaGpMl",
        "outputId": "04cdaf95-0b0f-4404-bd0c-cd6c6a341a23"
      },
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.countplot(x='NewsReach',data=d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7faa6bbb7610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKFwDJSSiFaV"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "df['headlines'] = LabelEncoder().fit_transform(df['headlines'].astype('str'))\n",
        "df['Sentiment'] = LabelEncoder().fit_transform(df['Sentiment'].astype('str'))\n",
        "df['Sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTTPSdaYiKu-"
      },
      "source": [
        "y=d['NewsReach']\n",
        "X=d[['tweet', 'tweet_length', 'retweet_count', 'hashtags_count', 'negativewordcount', 'positivewordcount',\n",
        "       'sentimentscore', 'Sentiment']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nRMIPgJIi_u"
      },
      "source": [
        "import numpy as np\n",
        "import pylab as pl\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pickle\n",
        "import xgboost as xg\n",
        "from xgboost import XGBClassifier \n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from time import time\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.metrics import f1_score\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, precision_recall_curve, recall_score, precision_score, precision_recall_curve, roc_curve, auc\n",
        "from matplotlib import cm\n",
        "from prettytable import PrettyTable\n",
        "from time import time\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "Pkl_File = \"/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Tripura/HW10/RESULT.pkl\"  \n",
        "names = [\"Decision Tree\",\"Random Forest\",\"MLP Neural Net\",\"Nearest Neighbors\", \"SVM\", \"AdaBoost\"]\n",
        "classifiers = [\n",
        "      Pipeline([('clf',DecisionTreeClassifier(max_depth=5))]),\n",
        "      Pipeline([('clf',RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1))]),\n",
        "      Pipeline([('clf',MLPClassifier(alpha=1, max_iter=1000))]),\n",
        "      Pipeline([('clf',KNeighborsClassifier(2))]),\n",
        "      Pipeline([('clf',SVC(kernel=\"linear\", C=0.025))]),\n",
        "      Pipeline([('clf',AdaBoostClassifier())])\n",
        "  ]\n",
        "def Muller_Loop(names, classifiers, X, y):\n",
        "    \n",
        "    result = PrettyTable()\n",
        "    result.field_names = [\"Model\", \"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"]\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33)\n",
        "\n",
        "    max_score = 0.0\n",
        "    max_class = ''\n",
        " \n",
        "    # Iterate over classifiers\n",
        "    for name, clf in zip(names, classifiers):\n",
        "        start_time = time()\n",
        "        clf.fit(X_train, y_train)\n",
        "        score = 100.0 * clf.score(X_test, y_test)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        f1Score = f1_score(y_test, y_pred, average=\"macro\")\n",
        "        precision = precision_score(y_test,y_pred,average='macro')\n",
        "        recall = recall_score(y_test, y_pred, average='macro')\n",
        "        if(name == \"Decision Tree\"):\n",
        "                  print('Saving model in pickle')\n",
        "                  with open(Pkl_File, 'wb') as file:  \n",
        "                    pickle.dump(clf, file)\n",
        "        print('Classifier = %s, Score (test, accuracy) = %.2f,' %(name, score), 'Training time = %.2f seconds' % (time() - start_time))\n",
        "        result.add_row([name, score, f1Score, precision, recall])\n",
        "       \n",
        "        if score > max_score:\n",
        "            clf_best = clf\n",
        "            max_score = score\n",
        "            max_class = name\n",
        "    \n",
        "    print(80*'-' )\n",
        "    print('Best --> Classifier = %s, Score (test, accuracy) = %.2f' %(max_class, max_score))\n",
        "    print(result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcJwJj18i1Aq"
      },
      "source": [
        "Muller_Loop(names, classifiers, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}